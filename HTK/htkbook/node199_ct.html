<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Class-based models</TITLE>
<META NAME="description" CONTENT="Contents of Class-based models">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="next" HREF="node200_mn.html">
<LINK REL="previous" HREF="node198_mn.html">
<LINK REL="up" HREF="node192_mn.html">
<LINK REL="next" HREF="node200_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H1><A NAME="SECTION04270000000000000000">&#160;</A><A NAME="s:HLMclassModels">&#160;</A>      

<A NAME="22719">&#160;</A>
<BR>
Class-based models
</H1>
A class-based <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram model is similar to a word-based <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram in
that both store probabilities <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-tuples of tokens - except in the
class model case these tokens consist of word <I>classes</I> instead of
words (although word models typically include at least one class for
the unknown word).  Thus building a class model involves constructing
class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-grams.  A second component of the model calculates the
probability of a word given each class.  The HTK tools only support
deterministic class maps, so each word can only be in one class.
Class language models use a separate file to store each of the two
components - the word-given-class probabilities and the class
<SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-grams - as well as a third file which points to the two component
files.  Alternatively, the two components can be combined together
into a standalone separate file.  In this section we'll see how to
build these files using the supplied tools.

<P>
Before a class model can be built it is necessary to construct a class
map which defines which words are in each class.  The supplied
C<SMALL>LUSTER</SMALL> tool can derive a class map based on the bigram word
statistics found in some text, although if you are constructing a
large number of classes it can be rather slow (execution time measured
in hours, typically).  In many systems class models are combined with
word models to give further gains, so we'll build a class model based
on the Holmes training text and then interpolate it with our existing
word model to see if we can get a better overall model.

<P>
Constructing a class map requires a decision to be made as to how many
separate classes are required.  A sensible number depends on what you
are building the model for, and whether you intend it purely to
interpolate with a word model.  In the latter case, for example, a
sensible number of classes is often around the 1000 mark when using a
64K word vocabulary.  We only have 5000 words in our vocabulary so
we'll choose to construct 150 classes in this case.

<P>
Create a directory called <TT>holmes.2</TT> and run C<SMALL>LUSTER</SMALL> with
<PRE>
$ Cluster -T 1 -c 150 -i 1 -k -o holmes.2/class lm_5k/5k.wmap
         holmes.1/data.* lm_5k/data.0
Preparing input gram set
Input gram file holmes.1/data.0 added (weight=1.000000)
Input gram file lm_5k/data.0 added (weight=1.000000)
Beginning iteration 1
Iteration complete
Cluster completed successfully
</PRE> The word map and gram files are passed as before - any OOV mapping
should be made before building the class map.  Passing the <TT>-k</TT>
option told C<SMALL>LUSTER</SMALL> to keep the unknown word token <TT>!!UNK</TT>
in its own singleton class, whilst the <TT>-c 150</TT> options specifies
that we wish to create 150 classes.  The <TT>-i 1</TT> performs only one
iteration of the clusterer - performing further iterations is likely
to give further small improvements in performance, but we won't wait
for this here.  Whilst C<SMALL>LUSTER</SMALL> is running you can look at the
end of the <TT>holmes.2/class.1.log</TT> to see how far it has got.  On a
Unix-like system you could use a command like <TT>tail
holmes.2/class.1.log</TT>, or if you wanted to monitor progress then <TT>tail -f holmes.2/class.1.log</TT> would do the trick.  The <TT>1</TT> refers
to the iteration, whilst the results are written to this filename
because of the <TT>-o holmes.2/class</TT> option which sets the prefix
for all output files.

<P>
In the <TT>holmes.2</TT> directory you will also see the files <TT>class.recovery</TT> and <TT>class.recovery.cm</TT> - these are a recovery
status file and its associated class map which are exported at regular
intervals because the C<SMALL>LUSTER</SMALL> tool can take so long to run.
In this way you can kill the tool before it has finished and resume
execution at a later date by using the <TT>-x</TT> option; in this case
you would use <TT>-x holmes.2/class.recovery</TT> for example (making
sure you pass the same word map and gram files - the tool does
<I>not</I> currently check that you pass it the same files when restarting).

<P>
Once the tool finishes running you should see the file <TT>holmes.2/class.1.cm</TT> which is the resulting class map.  It is in plain
text format so feel free to examine it.  Note, for example, how <TT>CLASS23</TT> consists almost totally of verb forms ending in <TT>-ED</TT>,
whilst <TT>CLASS41</TT> lists various general words for a person or
object.  Had you created more classes then you would be likely to see
more distinctive classes.  We can now use this file to build the class
<SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram component of our language model.
<PRE>
$ LGCopy -T 1 -d holmes.2 -m holmes.2/cmap -w holmes.2/class.1.cm
         lm_5k/5k.wmap holmes.1/data.* lm_5k/data.0
Input file holmes.1/data.0 added, weight=1.0000
Input file lm_5k/data.0 added, weight=1.0000
Copying 2 input files to output files with 2000000 entries
Class map = holmes.2/class.1.cm
 saving 162397 ngrams to file holmes.2/data.0 
330433 out of 330433 ngrams stored in 1 files
</PRE> 
<P>
The <TT>-w</TT> option specifies an input class map which is applied when
copying the gram files, so we now have a class gram file in <TT>holmes.2/data.0</TT>.  It has an associated word map file <TT>holmes.2/cmap</TT> - although this only contains class names it is
technically a word map since it is taken as input wherever a word map
is required by the HTK language modelling tools; recall that word
maps can contain classes as witnessed by <TT>!!UNK</TT> previously.

<P>
You can examine the class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-grams in a similar way to previously by
using LGL<SMALL>IST</SMALL>
<PRE>
$ LGList holmes.2/cmap holmes.2/data.0 | more 
 
3-Gram File holmes.2/data.0[162397 entries]:
 Text Source: LGCopy
CLASS1      CLASS10     CLASS103     : 1
CLASS1      CLASS10     CLASS11      : 2
CLASS1      CLASS10     CLASS118     : 1
CLASS1      CLASS10     CLASS12      : 1
CLASS1      CLASS10     CLASS126     : 2
CLASS1      CLASS10     CLASS140     : 2
CLASS1      CLASS10     CLASS147     : 1
...
</PRE> 
<P>
And similarly the class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram component of the overall language
model is built using LB<SMALL>UILD</SMALL> as previously with
<PRE>
$ LBuild -T 1 -c 2 1 -c 3 1 -n 3 holmes.2/cmap
     lm_5k/cl150-tg_1_1.cc holmes.2/data.*
Input file holmes.2/data.0 added, weight=1.0000
</PRE> 
<P>
To build the word-given-class component of the model we must run
C<SMALL>LUSTER</SMALL> again.
<PRE>
$ Cluster -l holmes.2/class.1.cm -i 0 -q lm_5k/cl150-counts.wc
     lm_5k/5k.wmap holmes.1/data.* lm_5k/data.0
</PRE> 
<P>
This is very similar to how we ran C<SMALL>LUSTER</SMALL> earlier, except
that we now want to perform 0 iterations (<TT>-i 0</TT>) and we start by
loading in the existing class map with <TT>-l holmes.2/class.1.cm</TT>.
We don't need to pass <TT>-k</TT> because we aren't doing any further
clustering and we don't need to specify the number of classes since
this is read from the class map along with the class contents.  The
<TT>-q lm_5k/cl150-counts.wc</TT> option tells the tool to write 
word-given-class counts to the specified file.  Alternatively we could
have specified <TT>-p</TT> instead of <TT>-q</TT> and written probabilities
as opposed to counts.  The file is in a plain text format, and either
the <TT>-p</TT> or <TT>-q</TT> version is sufficient for forming the
word-given-class component of a class language model.  Note that in
fact we could have simply added either <TT>-p</TT> or <TT>-q</TT> the
first time we ran C<SMALL>LUSTER</SMALL> and generated both the class map and
language model component file in one go.

<P>
Given the two language model components we can now link them together
to make our overall class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram language model.
<PRE>
$ LLink lm_5k/cl150-counts.wc lm_5k/cl150-tg_1_1.cc
     lm_5k/cl150-tg_1_1
</PRE> 
<P>
The LL<SMALL>INK</SMALL> tool creates a simple text file pointing to the two
necessary components, auto-detecting whether a count or probabilities
file has been supplied.  The resulting file, <TT>lm_5k/cl150-tg_1_1</TT>
is the finished overall class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram model, which we can now assess
the performance of with LP<SMALL>LEX</SMALL>.
<PRE>
$ LPlex -n 3 -t lm_5k/cl150-tg_1_1 test/red-headed_league.txt
LPlex test #0: 3-gram
perplexity 125.9065, var 7.4139, utterances 556, words predicted 8127
num tokens 10408, OOV 665, OOV rate 6.75% (excl. &lt;/s&gt;)

Access statistics for lm_5k/cl150-tg_1_1:
Lang model  requested  exact backed    n/a     mean    stdev
    bigram       2867  95.4%   4.6%   0.0%    -4.61     1.64
   trigram       8127  64.7%  24.1%  11.2%    -4.84     2.72
</PRE> 
<P>
The class trigram model performs worse than the word trigram (which
had a perplexity of 117.4), but this is not a surprise since this is
true of almost every reasonably-sized test set - the class model is
less specific.  Interpolating the two often leads to further
improvements, however.  We can find out if this will happen in this
case by interpolating the models with LP<SMALL>LEX</SMALL>.
<PRE>
$ LPlex -u -n 3 -t -i 0.4 lm_5k/cl150-tg_1_1 lm_5k/tg1_1
      test/red-headed_league.txt
LPlex test #0: 3-gram
perplexity 102.6389, var 7.3924, utterances 556, words predicted 9187
num tokens 10408, OOV 665, OOV rate 6.75% (excl. &lt;/s&gt;)
 
Access statistics for lm_5k/tg2-1_1:
Lang model  requested  exact backed    n/a     mean    stdev
    bigram       5911  68.5%  30.9%   0.6%    -5.75     2.94
   trigram       9187  35.7%  31.2%  33.2%    -4.77     2.98
 
Access statistics for lm_5k/cl150-tg_1_1:
Lang model  requested  exact backed    n/a     mean    stdev
    bigram       3104  95.5%   4.5%   0.0%    -4.67     1.62
   trigram       9187  66.2%  23.9%   9.9%    -4.87     2.75
</PRE> So a further gain is obtained - the interpolated model performs
significantly better.  Further improvement might be possible by
attempting to optimise the interpolation weight.

<P>
Note that we could also have used LL<SMALL>INK</SMALL> to build a single
class language model file instead of producing a third file which
points to the two components.  We can do this by using the <TT>-s</TT>
single file option.
<PRE>
$ LLink -s lm_5k/cl150-counts.wc lm_5k/cl150-tg_1_1.cc
     lm_5k/cl150-tg_1_1.all
</PRE> The file <TT>lm_5k/cl150-tg_1_1.all</TT> is now a standalone language
model, identical in performance to <TT>lm_5k/cl150-tg_1_1</TT> created
earlier.

<P>

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
