<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Equivalence classes</TITLE>
<META NAME="description" CONTENT="Contents of Equivalence classes">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="next" HREF="node180_mn.html">
<LINK REL="previous" HREF="node178_mn.html">
<LINK REL="up" HREF="node177_mn.html">
<LINK REL="next" HREF="node180_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H2><A NAME="SECTION04112000000000000000">&#160;</A><A NAME="s:HLMequivalenceclasses">&#160;</A>
<BR>
Equivalence classes
</H2>

The word <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram model described in equation <A HREF="node178_ct.html#ngram_model">14.2</A> uses
an equivalence mapping on the word history which assumes that all
contexts which have the same most recent <SPAN CLASS="MATH"><IMG
 WIDTH="41" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img212.png"
 ALT="$ n-1$"></SPAN> words all have the same
probability. This concept can be expressed more generally by defining
an equivalence class function that acts on word histories, <!-- MATH
 $\mathcal
E(.)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="31" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img499.png"
 ALT="$ \mathcal
E(.)$"></SPAN>, such that if <!-- MATH
 $\mathcal E(x) = \mathcal E(y)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="87" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img500.png"
 ALT="$ \mathcal E(x) = \mathcal E(y)$"></SPAN> then <!-- MATH
 $\forall w:$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="34" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img501.png"
 ALT="$ \forall w:$"></SPAN>
<!-- MATH
 $P(w | x) = P(w | y)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="125" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img502.png"
 ALT="$ P(w \vert x) = P(w \vert y)$"></SPAN>:
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="equiv_cond_prob_model">&#160;</A><!-- MATH
 \begin{equation}
P(w_i \;|\; w_1, w_2, \ldots, w_{i-1}) =
P(w_i \;|\; \mathcal E(w_1, w_2, \ldots, w_{i-1}))
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="382" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img503.png"
 ALT="$\displaystyle P(w_i \;\vert\; w_1, w_2, \ldots, w_{i-1}) = P(w_i \;\vert\; \mathcal E(w_1, w_2, \ldots, w_{i-1}))$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">4</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
A definition of <!-- MATH
 $\mathcal{E}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img504.png"
 ALT="$ \mathcal{E}$"></SPAN> that describes a word <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram is thus:
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
\mathcal E_{\textrm{word-{\it n}-gram}}(w_1, \ldots, w_{i}) = \mathcal E(w_{i-n+1}, \ldots, w_{i})
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="314" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img505.png"
 ALT="$\displaystyle \mathcal E_{\textrm{word-{\it n}-gram}}(w_1, \ldots, w_{i}) = \mathcal E(w_{i-n+1}, \ldots, w_{i})$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">5</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
In a good language model the choice of <!-- MATH
 $\mathcal{E}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img504.png"
 ALT="$ \mathcal{E}$"></SPAN> should be such that it
provides a reliable predictor of the next word, resulting in classes
which occur frequently enough in the training text that they can be
well modelled, and does not result in so many distinct history
equivalence classes that it is infeasible to store or analyse all the
resultant separate probabilities.

<P>

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
