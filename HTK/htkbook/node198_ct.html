<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Model interpolation</TITLE>
<META NAME="description" CONTENT="Contents of Model interpolation">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="next" HREF="node199_mn.html">
<LINK REL="previous" HREF="node197_mn.html">
<LINK REL="up" HREF="node192_mn.html">
<LINK REL="next" HREF="node199_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H1><A NAME="SECTION04260000000000000000">&#160;</A><A NAME="s:HLMmodelinterp">&#160;</A>      

<A NAME="22682">&#160;</A>
<BR>
Model interpolation
</H1>
The HTK language modelling tools also provide the capabilities to
produce and evaluate interpolated language models.  Interpolated
models are generated by combining a number of existing models in a
specified ratio to produce a new model using the tool LM<SMALL>ERGE</SMALL>.
Furthermore, LP<SMALL>LEX</SMALL> can also compute perplexities using
linearly interpolated <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram probabilities from a number of source
models.  The use of model interpolation will be demonstrated by
combining the previously generated Sherlock Holmes model with an
existing 60,000 word business news domain trigram model
(<TT>60bn_tg.lm</TT>).  The perplexity measure of the unseen Sherlock
Holmes text using the business news model is 297 with an OOV rate of
1.5%.  (<TT>LPlex -t -u 60kbn_tg.lm test/*</TT>). In the following
example, the perplexity of the test date will be calculated by
combining the two models in the ratio of 0.6 <TT>60kbn_tg.lm</TT> and
0.4 <TT>tg1_1c</TT>:
<PRE>
$ LPlex -T 1 -u -n 3 -t -i 0.6 ./60kbn_tg.lm 
        lm_5k/tg1_1c test/red-headed_league.txt
Loading language model from lm_5k/tg1_1c
Loading language model from ./60kbn_tg.lm
Using language model(s): 
  3-gram lm_5k/tg1_1c, weight 0.40
  3-gram ./60kbn_tg.lm, weight 0.60
Found 60275 unique words in 2 model(s)
LPlex test #0: 3-gram
Processing text stream: test/red-headed_league.txt
perplexity 188.0937, var 11.2408, utterances 556, words predicted 9721
num tokens 10408, OOV 131, OOV rate 1.33% (excl. &lt;/s&gt;)

Access statistics for lm_5k/tg1_1c:
Lang model  requested  exact backed    n/a     mean    stdev
    bigram       5479  68.0%  31.3%   0.6%    -5.69     2.93
   trigram       8329  34.2%  30.6%  35.1%    -4.75     2.99

Access statistics for ./60kbn_tg.lm:
Lang model  requested  exact backed    n/a     mean    stdev
    bigram       5034  83.0%  17.0%   0.1%    -7.14     3.57
   trigram       9683  48.0%  26.9%  25.1%    -5.69     3.53
</PRE> 
<P>
A single combined model can be generated using LM<SMALL>ERGE</SMALL>:
<PRE>
$ LMerge -T 1 -i 0.6 ./60kbn_tg.lm 5k_unk.wlist 
       lm_5k/rtg1_1 5k_merged.lm
</PRE> Note that LM<SMALL>ERGE</SMALL> cannot merge count-based models, hence the
use of <TT>lm_5k/rtg1_1</TT> instead of its count-based equivalent
<TT>lm_5k/tg1_1c</TT>.  Furthermore, the word list supplied to the
tool also includes the OOV symbol (<TT>!!UNK</TT>) in order to
preserve OOV <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-grams in the output model which in turn allows the
use of the <TT>-u</TT> option in LP<SMALL>LEX</SMALL>.

<P>
Note that the perplexity you will obtain with this combined model is
much lower than that when interpolating the two together because the
word list has been reduced from the union of the 60K and 5K ones down
to a single 5K list.  You can build a 5K version of the 60K model
using HLMC<SMALL>OPY</SMALL> and the <TT>-w</TT> option, but first you need to
construct a suitable word list - if you pass it the <TT>5k_unk.wlist</TT> one it will complain about the words in it that weren't
found in the language model.  In the <TT>extras</TT> subdirectory you'll
find a Perl script to rip the word list from the <TT>60kbn_tg.lm</TT>
model, <TT>getwordlist.pl</TT>, and the result of running it in <TT>60k.wlist</TT> (the script will work with any ARPA type language model).
The intersection of the 60K and 5K word lists is what is required, so
if you then run the <TT>extras/intersection.pl</TT> Perl script, amended
to use suitable filenames, you'll get the result in <TT>60k-5k-int.wlist</TT>.  Then HLMC<SMALL>OPY</SMALL> can be used to produce a 5K
vocabulary version of the 60K model:
<PRE>
$ HLMCopy -T 1 -w 60k-5k-int.wlist 60kbn_tg.lm 5kbn_tg.lm
</PRE> This can then be linearly interpolated with the previous 5K model to
compare the perplexity result with that obtained from the
LM<SMALL>ERGE</SMALL>-generated model.  If you try this you will find that
the perplexities are similar, but not exactly the same (a perplexity
of 112 with the merged model and 114 with the two models linearly
interpolated, in fact) - this is because using LM<SMALL>ERGE</SMALL> to
combine two models and then using the result is not precisely the same
as linearly interpolating two separate models; it is similar, however.

<P>
It is also possible to add to an existing language model using the
LA<SMALL>DAPT</SMALL> tool, which will construct a new model using supplied
text and then merge it with the existing one in exactly the same way
as LM<SMALL>ERGE</SMALL>.  Effectively this tool allows you to short-cut the
process by performing many operations with a single command - see the
documentation in section <A HREF="node364_ct.html#s:LAdapt">17.22</A> for full details.

<P>

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
