<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Isolated Word Recognition</TITLE>
<META NAME="description" CONTENT="Contents of Isolated Word Recognition">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="next" HREF="node6_mn.html">
<LINK REL="previous" HREF="node4_mn.html">
<LINK REL="up" HREF="node3_mn.html">
<LINK REL="next" HREF="node6_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H1><A NAME="SECTION02120000000000000000">&#160;</A><A NAME="s:isowrdrec">&#160;</A>
<BR>
Isolated Word Recognition
</H1>

<P>
Let each spoken word be represented by a sequence of speech vectors or <I>observations</I> <!-- MATH
 ${\mbox{\boldmath $O$}}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.png"
 ALT="$ {\mbox{\boldmath $O$}}$"></SPAN>,  defined as
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\mbox{\boldmath$O$}} = {\mbox{\boldmath$o$}}_1, {\mbox{\boldmath$o$}}_2, \ldots, {\mbox{\boldmath$o$}}_T
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="134" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img5.png"
 ALT="$\displaystyle {\mbox{\boldmath$O$}} = {\mbox{\boldmath$o$}}_1, {\mbox{\boldmath$o$}}_2, \ldots, {\mbox{\boldmath$o$}}_T$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">1</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <!-- MATH
 ${\mbox{\boldmath $o$}}_t$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img6.png"
 ALT="$ {\mbox{\boldmath $o$}}_t$"></SPAN> is the speech vector observed at time <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img7.png"
 ALT="$ t$"></SPAN>.  The
isolated word recognition problem can then be regarded as that of
computing
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:2">&#160;</A><!-- MATH
 \begin{equation}
\arg\max_i \left\{ P(w_i | {\mbox{\boldmath$O$}}) \right\}
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="137" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$\displaystyle \arg\max_i \left\{ P(w_i \vert {\mbox{\boldmath$O$}}) \right\}$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">2</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$ w_i$"></SPAN> is the <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img10.png"
 ALT="$ i$"></SPAN>'th vocabulary word.  This probability
is not computable directly but using Bayes' Rule<A NAME="1029">&#160;</A> gives
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:3">&#160;</A><!-- MATH
 \begin{equation}
P(w_i | {\mbox{\boldmath$O$}}) = \frac{P({\mbox{\boldmath$O$}}|w_i) P(w_i)}{P({\mbox{\boldmath$O$}})}
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="190" HEIGHT="53" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.png"
 ALT="$\displaystyle P(w_i \vert {\mbox{\boldmath$O$}}) = \frac{P({\mbox{\boldmath$O$}}\vert w_i) P(w_i)}{P({\mbox{\boldmath$O$}})}$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">3</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
Thus, for a given set of prior probabilities <SPAN CLASS="MATH"><IMG
 WIDTH="46" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$ P(w_i)$"></SPAN>, the most
probable spoken word depends only on the likelihood <!-- MATH
 $P({\mbox{\boldmath $O$}}|w_i)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="64" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$ P({\mbox{\boldmath $O$}}\vert w_i) $"></SPAN>.
Given the dimensionality of the observation sequence <!-- MATH
 ${\mbox{\boldmath $O$}}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.png"
 ALT="$ {\mbox{\boldmath $O$}}$"></SPAN>, the
direct estimation of the joint conditional probability 
<!-- MATH
 $P({\mbox{\boldmath $o$}}_1,{\mbox{\boldmath $o$}}_2,\ldots | w_i)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="118" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img14.png"
 ALT="$ P({\mbox{\boldmath $o$}}_1,{\mbox{\boldmath $o$}}_2,\ldots \vert w_i)$"></SPAN> from examples of spoken words
is not practicable. However, if a parametric model of word production
such as a Markov model
is assumed, then estimation from data is possible since the problem
of estimating the class conditional observation densities <!-- MATH
 $P({\mbox{\boldmath $O$}}|w_i)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="64" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$ P({\mbox{\boldmath $O$}}\vert w_i) $"></SPAN>
is replaced by the much simpler problem of estimating the Markov
model parameters.

<P>

<P>
<DIV ALIGN="CENTER">
<A NAME="f:isoprob">&#160;</A><IMG
 WIDTH="231" HEIGHT="258" ALIGN="MIDDLE" BORDER="0"
 SRC="img15.png"
 ALT="% latex2html id marker 48364
$\textstyle \parbox{50mm}{ \begin{center}\setlength...
...echapter.\arabic{figctr}  Isolated Word Problem}
\end{center}\end{center} }$">
</DIV>

<P>
I
n HMM based speech recognition, it is assumed that the sequence of
observed speech vectors corresponding to each word is generated
by a Markov model<A NAME="1045">&#160;</A> as shown in Fig.&nbsp;<A HREF="#_" TARGET="_top">[*]</A>.
A Markov model is a finite state machine which changes state
once every time unit and each time <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img7.png"
 ALT="$ t$"></SPAN> that a state <SPAN CLASS="MATH"><IMG
 WIDTH="12" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img16.png"
 ALT="$ j$"></SPAN> is entered, a
speech vector <!-- MATH
 ${\mbox{\boldmath $o$}}_t$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img6.png"
 ALT="$ {\mbox{\boldmath $o$}}_t$"></SPAN> is generated from the probability density
<!-- MATH
 $b_j({\mbox{\boldmath $o$}}_t)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="45" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img17.png"
 ALT="$ b_j({\mbox{\boldmath $o$}}_t)$"></SPAN>.  Furthermore, the transition from state <SPAN CLASS="MATH"><IMG
 WIDTH="10" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img10.png"
 ALT="$ i$"></SPAN> to state <SPAN CLASS="MATH"><IMG
 WIDTH="12" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img16.png"
 ALT="$ j$"></SPAN>
is also probabilistic and is governed by the discrete probability <SPAN CLASS="MATH"><IMG
 WIDTH="24" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img18.png"
 ALT="$ a_{ij}$"></SPAN>.
Fig.&nbsp;<A HREF="#_" TARGET="_top">[*]</A> shows an example of this process where the six state
model moves through the state sequence <!-- MATH
 $X=1,2,2,3,4,4,5,6$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="153" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img19.png"
 ALT="$ X=1,2,2,3,4,4,5,6$"></SPAN> in
order to generate the sequence <!-- MATH
 ${\mbox{\boldmath $o$}}_1$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$ {\mbox{\boldmath $o$}}_1$"></SPAN> to <!-- MATH
 ${\mbox{\boldmath $o$}}_6$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img21.png"
 ALT="$ {\mbox{\boldmath $o$}}_6$"></SPAN>. Notice that
in HTK, the entry and exit states of a HMM are non-emitting.  This
is to facilitate the construction of composite models as explained in
more detail later.

<P>
The joint probability that <!-- MATH
 ${\mbox{\boldmath $O$}}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.png"
 ALT="$ {\mbox{\boldmath $O$}}$"></SPAN> is generated by the model <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img22.png"
 ALT="$ M$"></SPAN> moving
through the state sequence
<SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img23.png"
 ALT="$ X$"></SPAN> is calculated simply as the product of the transition
probabilities and the output probabilities.  So for the state sequence <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img23.png"
 ALT="$ X$"></SPAN> in
Fig.&nbsp;<A HREF="#_" TARGET="_top">[*]</A>
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:4">&#160;</A><!-- MATH
 \begin{equation}
P({\mbox{\boldmath$O$}},X|M) = a_{12} b_2({\mbox{\boldmath$o$}}_1) a_{22} b_2({\mbox{\boldmath$o$}}_2) a_{23}
b_3({\mbox{\boldmath$o$}}_3) \ldots
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="322" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img24.png"
 ALT="$\displaystyle P({\mbox{\boldmath$O$}},X\vert M) = a_{12} b_2({\mbox{\boldmath$o...
... a_{22} b_2({\mbox{\boldmath$o$}}_2) a_{23} b_3({\mbox{\boldmath$o$}}_3) \ldots$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">4</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
However, in practice, only the observation sequence 
<!-- MATH
 ${\mbox{\boldmath $O$}}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="18" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img4.png"
 ALT="$ {\mbox{\boldmath $O$}}$"></SPAN> is known and the
underlying state sequence <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img23.png"
 ALT="$ X$"></SPAN> is hidden.  This is why it is
called a <I>Hidden Markov Model</I>.  

<P>

<P>
<DIV ALIGN="CENTER">
<A NAME="f:markovgen">&#160;</A><IMG
 WIDTH="389" HEIGHT="345" ALIGN="MIDDLE" BORDER="0"
 SRC="img25.png"
 ALT="% latex2html id marker 48399
$\textstyle \parbox{85mm}{ \begin{center}\setlength...
...er.\arabic{figctr}  The Markov Generation Model}
\end{center}\end{center} }$">
</DIV>

<P>

<P>
Given that <SPAN CLASS="MATH"><IMG
 WIDTH="19" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img23.png"
 ALT="$ X$"></SPAN> is unknown, the
required likelihood<A NAME="1070">&#160;</A> is computed 
by summing over all possible state
sequences <!-- MATH
 $X = x(1), x(2), x(3), \ldots, x(T)$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="210" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$ X = x(1), x(2), x(3), \ldots, x(T)$"></SPAN>, that is
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:5">&#160;</A><!-- MATH
 \begin{equation}
P({\mbox{\boldmath$O$}}|M) = \sum_X a_{x(0)x(1)} \prod_{t=1}^T b_{x(t)}({\mbox{\boldmath$o$}}_t)
a_{x(t)x(t+1)}
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="320" HEIGHT="65" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.png"
 ALT="$\displaystyle P({\mbox{\boldmath$O$}}\vert M) = \sum_X a_{x(0)x(1)} \prod_{t=1}^T b_{x(t)}({\mbox{\boldmath$o$}}_t) a_{x(t)x(t+1)}$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">5</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <SPAN CLASS="MATH"><IMG
 WIDTH="34" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img28.png"
 ALT="$ x(0)$"></SPAN> is constrained to be the model entry state and <SPAN CLASS="MATH"><IMG
 WIDTH="64" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img29.png"
 ALT="$ x(T+1)$"></SPAN>
is constrained to be the model exit state.

<P>
As an alternative to equation&nbsp;<A HREF="node5_ct.html#e:5">1.5</A>, the likelihood can be
approximated by only considering the most likely state
sequence, that is
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:6">&#160;</A><!-- MATH
 \begin{equation}
\hat{P}({\mbox{\boldmath$O$}}|M) = \max_X \left\{
a_{x(0)x(1)} \prod_{t=1}^T b_{x(t)}({\mbox{\boldmath$o$}}_t) a_{x(t)x(t+1)}
        \right\}
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="352" HEIGHT="65" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$\displaystyle \hat{P}({\mbox{\boldmath$O$}}\vert M) = \max_X \left\{ a_{x(0)x(1)} \prod_{t=1}^T b_{x(t)}({\mbox{\boldmath$o$}}_t) a_{x(t)x(t+1)} \right\}$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">6</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
Although the direct computation of equations <A HREF="node5_ct.html#e:5">1.5</A> and <A HREF="node5_ct.html#e:6">1.6</A>
is not tractable, simple recursive procedures exist which allow
both quantities to be calculated very efficiently.
Before going any further, however, notice that if equation&nbsp;<A HREF="node5_ct.html#e:2">1.2</A> is
computable then the recognition problem is solved.  Given a set of models
<SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img31.png"
 ALT="$ M_i$"></SPAN> corresponding to words <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$ w_i$"></SPAN>, equation&nbsp;<A HREF="node5_ct.html#e:2">1.2</A> is
solved by using <A HREF="node5_ct.html#e:3">1.3</A> and assuming that
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="e:7">&#160;</A><!-- MATH
 \begin{equation}
P({\mbox{\boldmath$O$}}|w_i) = P({\mbox{\boldmath$O$}}|M_i).
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="153" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img32.png"
 ALT="$\displaystyle P({\mbox{\boldmath$O$}}\vert w_i) = P({\mbox{\boldmath$O$}}\vert M_i).$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">1</SPAN>.<SPAN CLASS="arabic">7</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
All this, of course, assumes that the parameters <!-- MATH
 $\{a_{ij}\}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="40" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img33.png"
 ALT="$ \{a_{ij}\}$"></SPAN> and
<!-- MATH
 $\{b_{j}({\mbox{\boldmath $o$}}_t)\}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="61" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img34.png"
 ALT="$ \{b_{j}({\mbox{\boldmath $o$}}_t)\}$"></SPAN> are known for each model <SPAN CLASS="MATH"><IMG
 WIDTH="25" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img31.png"
 ALT="$ M_i$"></SPAN>.  Herein lies the
elegance and power of the HMM framework.  Given a set of training examples
corresponding to a particular model, the parameters of that model can be
determined automatically by a robust and efficient re-estimation 
procedure.  Thus, provided that a sufficient number of representative
examples of each word can be collected then a HMM can be constructed
which implicitly models all of the many sources of variability inherent
in real speech.  Fig.&nbsp;<A HREF="#_" TARGET="_top">[*]</A> summarises the use of HMMs
for isolated word recognition.  Firstly, a
HMM is trained for each vocabulary word using a number of examples
of that word.  In this case, the vocabulary consists of
just three words: ``one'', ``two'' and ``three''.
Secondly, to recognise some unknown word, the likelihood of 
each model generating that word is calculated and the most likely
model identifies the word.

<P>

<P>
<DIV ALIGN="CENTER">
<A NAME="f:useforiso">&#160;</A><IMG
 WIDTH="384" HEIGHT="573" ALIGN="MIDDLE" BORDER="0"
 SRC="img35.png"
 ALT="% latex2html id marker 48424
$\textstyle \parbox{84mm}{ \begin{center}\setlength...
...gctr}  Using HMMs for Isolated Word Recognition}
\end{center}\end{center} }$">
</DIV>

<P>

<P>

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
