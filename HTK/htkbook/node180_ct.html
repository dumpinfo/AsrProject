<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Class n-gram models</TITLE>
<META NAME="description" CONTENT="Contents of Class n-gram models">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="previous" HREF="node179_mn.html">
<LINK REL="up" HREF="node177_mn.html">
<LINK REL="next" HREF="node181_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H2><A NAME="SECTION04113000000000000000">&#160;</A><A NAME="s:HLMclassngram">&#160;</A><A NAME="classngram-description">&#160;</A>
<BR>
Class <I>n</I>-gram models
</H2>
One method of reducing the number of word history equivalence classes
to be modelled in the <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram case is to consider some words as
equivalent. This can be implemented by mapping a set of words to a
word class <!-- MATH
 $g \in \mathbb{G}$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="44" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img506.png"
 ALT="$ g \in \mathbb{G}$"></SPAN> by using a classification function <SPAN CLASS="MATH"><IMG
 WIDTH="70" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img507.png"
 ALT="$ G(w)
= g$"></SPAN>. If any class contains more than one word then this mapping will
result in less distinct word classes than there are words,
<!-- MATH
 $|\mathbb{G}| < |\mathbb{W}|$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="71" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img508.png"
 ALT="$ \vert\mathbb{G}\vert &lt; \vert\mathbb{W}\vert$"></SPAN>, thus reducing the number of separate
contexts that must be considered. The equivalence classes can then be
described as a sequence of these classes:
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="equiv_classes">&#160;</A><!-- MATH
 \begin{equation}
\mathcal E_{\textrm{class-{\it n}-gram}}(w_1, \ldots, w_{i}) = \mathcal
E(G(w_{i-n+1}), \ldots, G(w_{i}))
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="362" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img509.png"
 ALT="$\displaystyle \mathcal E_{\textrm{class-{\it n}-gram}}(w_1, \ldots, w_{i}) = \mathcal E(G(w_{i-n+1}), \ldots, G(w_{i}))$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">6</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
A deterministic word-to-class mapping like this has some advantages
over a word <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram model since the reduction in the number of
distinct histories reduces the storage space and training data
requirements whilst improving the robustness of the probability
estimates for a given quantity of training data. Because multiple
words can be mapped to the same class, the model has the ability to
make more confident assumptions about infrequent words in a class
based on other more frequent words in the same class<A NAME="tex2html58" HREF="footnode_mn.html#foot21343" TARGET="footer"><SUP><SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">5</SPAN></SUP></A> than is possible in the word <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram case - and
furthermore for the same reason it is able to make generalising
assumptions about words used in contexts which are not explicitly
encountered in the training text. These gains, however, clearly
correspond with a loss in the ability to distinguish between different
histories, although this might be offset by the ability to
choose a higher value of <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>.

<P>
The most commonly used form of class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram model uses a single
classification function, <SPAN CLASS="MATH"><IMG
 WIDTH="33" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img510.png"
 ALT="$ G(.)$"></SPAN>, as in equation <A HREF="node180_ct.html#equiv_classes">14.6</A>,
which is applied to each word in the <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram, including the word which
is being predicted. Considering for clarity the bigram<A NAME="tex2html59" HREF="footnode_mn.html#foot21707" TARGET="footer"><SUP><SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">6</SPAN></SUP></A>case, then given <SPAN CLASS="MATH"><IMG
 WIDTH="33" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img510.png"
 ALT="$ G(.)$"></SPAN> the language model has the terms <SPAN CLASS="MATH"><IMG
 WIDTH="21" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$ w_i$"></SPAN>,
<SPAN CLASS="MATH"><IMG
 WIDTH="37" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img511.png"
 ALT="$ w_{i-1}$"></SPAN>, <SPAN CLASS="MATH"><IMG
 WIDTH="46" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img512.png"
 ALT="$ G(w_i)$"></SPAN> and <!-- MATH
 $G(w_{i-1})$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="62" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img513.png"
 ALT="$ G(w_{i-1})$"></SPAN> available to it. The probability
estimate can be decomposed as follows:
<BR>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="chap2equalToOne">&#160;</A><!-- MATH
 \begin{eqnarray}
P_{\textrm{class'}}(w_i \;|\; w_{i-1})
& = & P(w_i \;|\; G(w_i), G(w_{i-1}), w_{i-1} )\nonumber\\
  & \qquad\qquad\times & P(G(w_i) \;|\; G(w_{i-1}), w_{i-1})
\end{eqnarray}
 -->
<TABLE CELLPADDING="0" ALIGN="CENTER" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="120" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img514.png"
 ALT="$\displaystyle P_{\textrm{class'}}(w_i \;\vert\; w_{i-1})$"></TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><IMG
 WIDTH="16" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img80.png"
 ALT="$\displaystyle =$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="205" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img515.png"
 ALT="$\displaystyle P(w_i \;\vert\; G(w_i), G(w_{i-1}), w_{i-1} )$"></TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><IMG
 WIDTH="80" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img516.png"
 ALT="$\displaystyle \qquad\qquad\times$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="182" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img517.png"
 ALT="$\displaystyle P(G(w_i) \;\vert\; G(w_{i-1}), w_{i-1})$"></TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
(<SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">7</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
It is assumed that <!-- MATH
 $P(w_i
\;|\; G(w_i), G(w_{i-1}), w_{i-1})$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="205" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img518.png"
 ALT="$ P(w_i
\;\vert\; G(w_i), G(w_{i-1}), w_{i-1})$"></SPAN> is independent of <!-- MATH
 $G(w_{i-1})$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="62" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img513.png"
 ALT="$ G(w_{i-1})$"></SPAN> and
<SPAN CLASS="MATH"><IMG
 WIDTH="37" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img511.png"
 ALT="$ w_{i-1}$"></SPAN> and that <!-- MATH
 $P(G(w_i) \;|\; G(w_{i-1}), w_{i-1})$
 -->
<SPAN CLASS="MATH"><IMG
 WIDTH="182" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img519.png"
 ALT="$ P(G(w_i) \;\vert\; G(w_{i-1}), w_{i-1})$"></SPAN> is
independent of <SPAN CLASS="MATH"><IMG
 WIDTH="37" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img511.png"
 ALT="$ w_{i-1}$"></SPAN>, resulting in the model:
<P></P>
<DIV ALIGN="CENTER" CLASS="mathdisplay"><A NAME="normclass">&#160;</A><!-- MATH
 \begin{equation}
P_{\textrm{class}}(w_i \;|\; w_{i-1}) = P(w_i \;|\; G(w_{i})) \;\times\;
P(G(w_i) \;|\; G(w_{i-1}))
\end{equation}
 -->
<TABLE CLASS="equation" CELLPADDING="0" WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE">
<TD NOWRAP ALIGN="CENTER"><SPAN CLASS="MATH"><IMG
 WIDTH="399" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img520.png"
 ALT="$\displaystyle P_{\textrm{class}}(w_i \;\vert\; w_{i-1}) = P(w_i \;\vert\; G(w_{i})) \;\times\; P(G(w_i) \;\vert\; G(w_{i-1}))$"></SPAN></TD>
<TD NOWRAP CLASS="eqno" WIDTH="10" ALIGN="RIGHT">
(<SPAN CLASS="arabic">14</SPAN>.<SPAN CLASS="arabic">8</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P> 

<P>
Almost all reported class <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram work using statistically-found classes
is based on clustering algorithms which optimise <SPAN CLASS="MATH"><IMG
 WIDTH="33" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img510.png"
 ALT="$ G(.)$"></SPAN> on the basis
of bigram training set likelihood, even if the class map is to be used
with longer-context models.  It is interesting to
note that this approximation appears to works well, however,
suggesting that the class maps found are in some respects ``general''
and capture some features of natural language which apply irrespective
of the context length used when finding these features.

<P>

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
