<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Contents of Robust model estimation</TITLE>
<META NAME="description" CONTENT="Contents of Robust model estimation">
<META NAME="keywords" CONTENT="htkbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="htkbook.css">

<LINK REL="next" HREF="node189_mn.html">
<LINK REL="previous" HREF="node181_mn.html">
<LINK REL="up" HREF="node176_mn.html">
<LINK REL="next" HREF="node184_mn.html">
</HEAD>
 
<BODY bgcolor="#ffffff" text="#000000" link="#9944EE" vlink="#0000ff" alink="#00ff00">

<H1><A NAME="SECTION04130000000000000000">&#160;</A><A NAME="s:HLMrobustestimates">&#160;</A>      
<A NAME="robust_estimation">&#160;</A>
<BR>
Robust model estimation
</H1>
Given a suitably large amount of training data, an extremely long <SPAN CLASS="MATH"><IMG
 WIDTH="14" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ n$"></SPAN>-gram
could be trained to give a very good model of language, as per equation
<A HREF="node177_ct.html#cond_prob_model">14.1</A> - in practice, however,
any actual extant model must be an approximation. Because it is an
approximation, it will be detrimental to include within the model
information which in fact was just noise introduced by the limits of
the bounded sample set used to train the model - this information may
not accurately represent text not contained within the training
corpus. In the same way, word sequences which were not observed in the
training text cannot be assumed to represent impossible sequences, so
some probability mass must be reserved for these. The issue of how to
redistribute the probability mass, as assigned by the maximum likelihood
estimates derived from the raw statistics of a specific corpus, into a
sensible estimate of the real world is addressed by various standard
methods, all of which aim to create more robust language models.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL CLASS="ChildLinks">
<LI><A NAME="tex2html3834" HREF="node184_mn.html" TARGET="main"><SMALL>Estimating probabilities</SMALL></A>
<UL>
<LI><A NAME="tex2html3835" HREF="node185_mn.html" TARGET="main"><SMALL>Good-Turing discounting</SMALL></A>
<LI><A NAME="tex2html3836" HREF="node186_mn.html" TARGET="main"><SMALL>Absolute discounting</SMALL></A>
</UL>
<BR>
<LI><A NAME="tex2html3837" HREF="node187_mn.html" TARGET="main"><SMALL>Smoothing probabilities</SMALL></A>
<UL>
<LI><A NAME="tex2html3838" HREF="node188_mn.html" TARGET="main"><SMALL>Cut-offs</SMALL></A>
</UL></UL>
<!--End of Table of Child-Links-->

<HR>
<ADDRESS>
<A HREF=http://htk.eng.cam.ac.uk/docs/docs.shtml TARGET=_top>Back to HTK site</A><BR>See front page for HTK Authors
</ADDRESS>
</BODY>
</HTML>
